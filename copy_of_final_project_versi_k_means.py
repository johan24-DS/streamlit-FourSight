# -*- coding: utf-8 -*-
"""Copy of Final Project Versi K-Means

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-j58vU6oshtOrGSpOLejvvXAHTKnvLeG
"""

import gdown
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy import stats
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

from sklearn.cluster import KMeans
from scipy.stats import skew
from scipy.stats import boxcox
from sklearn.metrics import davies_bouldin_score
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

from yellowbrick.cluster import SilhouetteVisualizer
from yellowbrick.cluster import KElbowVisualizer

import warnings
warnings.filterwarnings('ignore')

file_id = '1kIXpkpjEmonYCn_VNw7a-2p1CflaAuoa'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
gdown.download(url, 'data.csv', quiet=False)

d_listing = pd.read_csv('data.csv')
d_listing.head(1)

file_id = '1X9a59SPipMZopcy2XJ7zvbfZNnjDnhtV'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
gdown.download(url, 'data.csv', quiet=False)

d_calender = pd.read_csv('data.csv')
d_calender.head()

calendar_selected = d_calender[['listing_id', 'date', 'available', 'price']]
listings_selected = d_listing[['id', 'property_type', 'room_type', 'price', 'city', 'review_scores_rating',
                                 'accommodates', 'bedrooms', 'beds', 'bed_type', 'latitude', 'longitude',
                                 'host_is_superhost', 'neighbourhood_cleansed', 'bathrooms', 'availability_365',
                                 'number_of_reviews', 'host_response_time']]

"""# Handling Missing Value"""

calendar_selected['price'].fillna(0, inplace=True)
print(calendar_selected.isnull().sum())

listings_selected['review_scores_rating'].fillna(listings_selected['review_scores_rating'].median(), inplace=True)
listings_selected['host_is_superhost'].fillna(listings_selected['host_is_superhost'].mode()[0], inplace=True)
listings_selected['property_type'].fillna(listings_selected['property_type'].mode()[0], inplace=True)
listings_selected['bathrooms'].fillna(listings_selected['bathrooms'].median(), inplace=True)
listings_selected['bedrooms'].fillna(listings_selected['bedrooms'].median(), inplace=True)
listings_selected['beds'].fillna(listings_selected['beds'].median(), inplace=True)
listings_selected['host_response_time'].fillna(listings_selected['host_response_time'].mode()[0], inplace=True)
print(listings_selected.isnull().sum())

"""# Handling Inconsistency"""

calendar_selected['price'] = calendar_selected['price'].replace({'\$': '', ',': ''}, regex=True)
calendar_selected['price'] = pd.to_numeric(calendar_selected['price'], errors='coerce')
print(calendar_selected['price'].head())

listings_selected['price'] = listings_selected['price'].replace({'\$': '', ',': ''}, regex=True)
listings_selected['price'] = pd.to_numeric(listings_selected['price'], errors='coerce')
print(listings_selected['price'].head())

for col in calendar_selected.select_dtypes(include=['object']).columns:
    calendar_selected[col] = calendar_selected[col].str.strip().str.title()

calendar_selected.head()

for col in listings_selected.select_dtypes(include=['object']).columns:
    listings_selected[col] = listings_selected[col].str.strip().str.title()

listings_selected.head

"""# Handling Outlier"""

def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = df[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if not pd.isnull(x) else x)

columns_to_handle = ['review_scores_rating', 'accommodates', 'bedrooms', 'beds', 'bathrooms', 'number_of_reviews']
for column in columns_to_handle:
    handle_outliers(listings_selected, column)

handle_outliers(calendar_selected, 'price')

import numpy as np
import pandas as pd

# Membuat dictionary untuk menyesuaikan nama kota yang ada pada dataset
city_mapping = {
    'Seattle': 'Seattle',
    'Phinney Ridge Seattle': 'Seattle',
    'West Seattle': 'Seattle',
    'Ballard, Seattle': 'Seattle',
    '西雅图': 'Seattle',  # 'Xīyátú' untuk Seattle dalam Mandarin
    # Jika ada nama kota lain dalam bahasa atau format berbeda, bisa ditambahkan di sini
}

# Mengganti nama kota di kolom 'city' dengan nama kota yang konsisten
listings_selected['city_normalized'] = listings_selected['city'].map(city_mapping).fillna(listings_selected['city'])

# Memeriksa nilai unik pada kolom 'city' untuk memverifikasi semua nama kota yang perlu dipetakan
print("Unique cities in the original 'city' column:")
print(listings_selected['city'].unique())

# Memeriksa nilai-nilai yang terdeteksi sebagai 'NaN' pada 'city_normalized'
print("Cities with 'NaN' in 'city_normalized' after mapping:")
print(listings_selected[listings_selected['city_normalized'].isna()]['city'].unique())

# Koordinat pusat kota untuk beberapa kota (contoh: Seattle)
city_coordinates = {
    'Seattle': (47.6062, -122.3321),  # Koordinat pusat kota Seattle
    # Tambahkan koordinat untuk kota lain jika ada dalam dataset
}

# Menambah koordinat untuk kota yang belum ada di city_coordinates
city_coordinates.update({
    'New York': (40.7128, -74.0060),
    'Los Angeles': (34.0522, -118.2437),
    # Tambahkan kota lainnya jika diperlukan
})

# Fungsi untuk mendapatkan koordinat kota
def get_city_coordinates(city_name):
    return city_coordinates.get(city_name, (None, None))

# Menambahkan kolom untuk latitude dan longitude pusat kota
listings_selected['city_lat'], listings_selected['city_lon'] = zip(*listings_selected['city_normalized'].apply(get_city_coordinates))

# Menghitung jarak ke pusat kota yang sesuai berdasarkan koordinat kota dan properti
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Radius bumi dalam kilometer
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c  # Jarak dalam kilometer

# Menghitung jarak ke pusat kota yang sesuai berdasarkan koordinat kota dan properti
listings_selected['distance_to_city'] = listings_selected.apply(
    lambda x: haversine(x['latitude'], x['longitude'], x['city_lat'], x['city_lon']) if x['city_lat'] and x['city_lon'] else None, axis=1
)

# Menampilkan hasil jarak ke pusat kota
print("Sample of the dataframe with calculated distance to city:")
#print(listings_selected[['listing_id', 'city_normalized', 'latitude', 'longitude', 'distance_to_city']].head())

listings_selected.columns



"""# Clustering Listing Data"""

listings_selected_copy = listings_selected.copy()
listings_selected_copy.drop(['id','city','latitude','longitude','neighbourhood_cleansed', 'city_lat', 'city_lon'],axis=1, inplace=True)
listings_selected_copy

listings_selected_copy.columns

superhost_mapping = {'T': 1, 'F': 0}
listings_selected_copy['host_is_superhost'] = listings_selected_copy['host_is_superhost'].map(superhost_mapping)

listings_selected_copy.info()

response_time_mapping = {
    'Within An Hour': 1,
    'Within A Few Hours': 2,
    'Within A Day': 3,
    'A Few Days Or More': 4
}
# Menghapus spasi ekstra di kedua sisi nilai dalam kolom 'host_response_time'
listings_selected_copy['host_response_time'] = listings_selected_copy['host_response_time'].str.strip()

# Kemudian lakukan pemetaan mapping)
listings_selected_copy['host_response_time_encoded'] = listings_selected_copy['host_response_time'].map(response_time_mapping)

# Menghitung statistik (mean, std, median) berdasarkan property_type dan price
property_stats = listings_selected_copy.groupby('property_type')['price'].agg(['mean', 'max', 'median'])
#property_stats = listings_selected_copy.groupby('property_type')['price_x'].agg(['mean', 'std', 'median'])

# Mengubah nama kolom untuk memperjelas arti statistik yang dihitung
property_stats.columns = ['mean_property_type', 'max_property_type', 'median_property_type']

# Menampilkan hasil statistik untuk property_type dengan nama kolom yang jelas
print(property_stats)

# Melakukan merge untuk menambahkan kolom mean, std, dan median ke listings_selected_copy
listings_selected_copy = listings_selected_copy.merge(property_stats, on='property_type', how='left')

listings_selected_copy.drop(['property_type', 'host_response_time'],axis=1, inplace=True)

listings_selected_copy.columns

listings_selected_copy

one_hot_cat_feature = ['room_type','bed_type']
numeric_feature = ['price', 'review_scores_rating', 'accommodates',
       'bedrooms', 'beds', 'host_is_superhost',
       'availability_365', 'number_of_reviews','distance_to_city', 'host_response_time_encoded',
       'mean_property_type', 'max_property_type', 'median_property_type']

numeric_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_feature),
        ('cat', categorical_transformer, one_hot_cat_feature)
    ]
)

pipeline = Pipeline(steps=[('preprocessor', preprocessor)])
pipeline

transformed_data = pipeline.fit_transform(listings_selected_copy)
feature_names = list(pipeline.named_steps['preprocessor'].named_transformers_['num'].get_feature_names_out(numeric_feature))
feature_names += list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(one_hot_cat_feature))

transformed_data = pd.DataFrame(transformed_data, columns=feature_names)
for col in transformed_data.columns:
  if col not in numeric_feature:
      transformed_data[col] = transformed_data[col].astype(int)

transformed_data

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
import matplotlib.pyplot as plt

# Inisialisasi MiniBatchKMeans
cluster_model = KMeans(random_state=42)

# Visualisasi menggunakan KElbowVisualizer untuk mencari jumlah klaster yang optimal
visualizer = KElbowVisualizer(cluster_model, k=(2, 10))
visualizer.fit(transformed_data.to_numpy())
visualizer.show()

# Menampilkan plot
plt.show()

from sklearn.cluster import KMeans

# Inisialisasi KMeans dengan jumlah klaster optimal yang diperoleh dari visualizer
model = KMeans(n_clusters=visualizer.elbow_value_, random_state=42)

# Fit dan prediksi labels menggunakan KMeans
labels = model.fit_predict(transformed_data.to_numpy())

# Mengubah label dari 0-based menjadi 1-based
labels = labels + 1

# Menampilkan labels yang diperoleh
print(f'Labels: {labels}')

print(f'Davies-Bouldin index = {davies_bouldin_score(transformed_data.to_numpy(), labels)}')
print(f'Silhouette Score     = {silhouette_score(transformed_data.to_numpy(), labels)}')

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Inisialisasi KMeans
cluster_model = KMeans(random_state=42)

# Tentukan jumlah klaster yang akan diuji
k_values = range(2, 11)  # Misalnya, kita uji jumlah klaster antara 3 dan 10
silhouette_scores = []

# Loop untuk menguji silhouette score dengan jumlah klaster yang berbeda
for k in k_values:
    cluster_model.n_clusters = k
    cluster_model.fit(transformed_data)  # Fit data pada model
    score = silhouette_score(transformed_data, cluster_model.labels_)  # Hitung silhouette score
    silhouette_scores.append(score)

# Plot silhouette scores
plt.figure(figsize=(8, 6))
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Silhouette Score untuk Setiap Jumlah Klaster')
plt.xlabel('Jumlah Klaster')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# Menampilkan nilai silhouette score terbaik
best_k = k_values[silhouette_scores.index(max(silhouette_scores))]

import pandas as pd

# Buat DataFrame untuk menyimpan hasil clustering dalam bentuk awal (standardized)
df_clustered = pd.DataFrame(transformed_data, columns=feature_names)  # Data sebelum inverse transform
df_clustered['cluster'] = labels  # Tambahkan hasil clustering

# Hitung rata-rata per klaster
profiling_result = df_clustered.groupby('cluster').mean()

# Tampilkan hasil
print(profiling_result)

import pandas as pd
from google.colab import files

file_id = '1kIXpkpjEmonYCn_VNw7a-2p1CflaAuoa'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
gdown.download(url, 'data.csv', quiet=False)

d_listing1 = pd.read_csv('data.csv')
d_listing1.head

import numpy as np
import pandas as pd

# Membuat dictionary untuk menyesuaikan nama kota yang ada pada dataset
city_mapping = {
    'Seattle': 'Seattle',
    'Phinney Ridge Seattle': 'Seattle',
    'West Seattle': 'Seattle',
    'Ballard, Seattle': 'Seattle',
    '西雅图': 'Seattle',  # 'Xīyátú' untuk Seattle dalam Mandarin
    # Jika ada nama kota lain dalam bahasa atau format berbeda, bisa ditambahkan di sini
}

# Mengganti nama kota di kolom 'city' dengan nama kota yang konsisten
d_listing1['city_normalized'] = d_listing1['city'].map(city_mapping).fillna(d_listing1['city'])

# Memeriksa nilai unik pada kolom 'city' untuk verifikasi
print("Unique cities in the original 'city' column:")
print(d_listing1['city'].unique())

# Memeriksa nilai yang terdeteksi sebagai 'NaN' pada 'city_normalized'
print("Cities with 'NaN' in 'city_normalized' after mapping:")
print(d_listing1[d_listing1['city_normalized'].isna()]['city'].unique())

# Koordinat pusat kota untuk beberapa kota (contoh: Seattle)
city_coordinates = {
    'Seattle': (47.6062, -122.3321),
    # Tambahkan koordinat kota lain jika ada
}

# Menambah koordinat untuk kota lainnya jika diperlukan
city_coordinates.update({
    'New York': (40.7128, -74.0060),
    'Los Angeles': (34.0522, -118.2437),
    # Tambahkan kota lain jika diperlukan
})

# Fungsi untuk mendapatkan koordinat kota
def get_city_coordinates(city_name):
    return city_coordinates.get(city_name, (None, None))

# Menambahkan kolom latitude dan longitude pusat kota
d_listing1['city_lat'], d_listing1['city_lon'] = zip(*d_listing1['city_normalized'].apply(get_city_coordinates))

# Fungsi Haversine untuk menghitung jarak ke pusat kota
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Radius bumi dalam km
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c  # Jarak dalam km

# Menghitung jarak ke pusat kota untuk setiap listing
d_listing1['distance_to_city'] = d_listing1.apply(
    lambda x: haversine(x['latitude'], x['longitude'], x['city_lat'], x['city_lon']) if x['city_lat'] and x['city_lon'] else None, axis=1
)

import pandas as pd
from google.colab import files
!pip install xlsxwriter

# Pastikan panjang dataframe sama sebelum menambahkan kolom cluster
if len(d_listing1) == len(df_clustered):
    d_listing1["cluster"] = df_clustered["cluster"].values  # Tambahkan kolom cluster

    # Sheet 2: Profiling hasil clustering
    profiling_result = df_clustered.groupby("cluster").mean()

    # Sheet 3: Profiling dari mean data asli (listings_selected_copy)
    listings_selected_copy["cluster"] = df_clustered["cluster"].values  # Tambahkan cluster ke data asli
    profiling_mean_original = listings_selected_copy.groupby("cluster").mean(numeric_only=True)

    # Simpan ke file Excel dengan multiple sheets
    file_path = "Hasil Clustering KMeans.xlsx"
    with pd.ExcelWriter(file_path, engine="xlsxwriter") as writer:
        d_listing1.to_excel(writer, sheet_name="Data_Clustered", index=False)  # Sheet 1
        profiling_result.to_excel(writer, sheet_name="Profiling_Result")  # Sheet 2
        profiling_mean_original.to_excel(writer, sheet_name="Profiling_Mean_Original")  # Sheet 3

    # Download ke lokal
    files.download(file_path)

    print(f"File berhasil disimpan dan diunduh: {file_path}")

else:
    print("Error: Panjang d_listing1 dan df_clustered tidak sama!")

transformed_data['label'] = labels
pca = PCA(n_components=3)

principal_components = pca.fit_transform(transformed_data.drop('label', axis=1))
principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])
principal_df['label'] = transformed_data['label']
principal_df

import pandas as pd

# Buat DataFrame untuk menyimpan hasil clustering dalam bentuk awal (standardized)
df_clustered = pd.DataFrame(transformed_data, columns=feature_names)  # Data sebelum inverse transform
df_clustered['cluster'] = labels  # Tambahkan hasil clustering

# Hitung rata-rata per klaster
profiling_result = df_clustered.groupby('cluster').mean()

# Tampilkan hasil
profiling_result

ax = principal_df.plot.scatter(
    x='PC1',
    y='PC2',
    s=principal_df['PC3'] * 20,
    c='label',
    colormap='magma'
)

centers = principal_df.groupby('label')[['PC1', 'PC2']].mean()
for label, (x, y) in centers.iterrows():
    plt.annotate(f'Cluster {label}', (x, y), textcoords="offset points", xytext=(5,5), ha='left', fontsize=10, color='black', weight='bold')

plt.title('Scatter Plot of Principal Components')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

listings_selected_copy['cluster'] = labels
listings_selected_copy

def plot_pie_chart(data):
    clusters = listings_selected_copy['cluster'].unique()
    num_clusters = len(clusters)

    rows = (num_clusters // 3) + (num_clusters % 3 > 0)
    cols = min(4, num_clusters)

    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))
    axes = np.array(axes).flatten()

    for i, cluster in enumerate(clusters):
        cluster_data = listings_selected_copy[listings_selected_copy['cluster'] == cluster]
        property_counts = cluster_data[data].value_counts()
        axes[i].pie(property_counts, labels=property_counts.index, autopct='%1.1f%%', startangle=140)
        axes[i].set_title(f'Cluster {cluster}')

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

def plot_histogram(data):
    clusters = listings_selected_copy['cluster'].unique()
    num_clusters = len(clusters)

    rows = (num_clusters // 3) + (num_clusters % 3 > 0)
    cols = min(4, num_clusters)

    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))
    axes = np.array(axes).flatten()

    for i, cluster in enumerate(clusters):
        cluster_data = listings_selected_copy[listings_selected_copy['cluster'] == cluster]
        sns.histplot(cluster_data, x=data, hue='cluster', kde=True, bins=20, ax=axes[i])
        axes[i].set_title(f'Cluster {cluster}')
        axes[i].set_xlabel(data)
        axes[i].set_ylabel('Frequency')

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

#plot_pie_chart('property_type')

plot_pie_chart('room_type')

plot_pie_chart('bed_type')

plot_pie_chart('room_type')

#plot_pie_chart('host_response_time')

plot_histogram('price')

plot_histogram('review_scores_rating')

plot_histogram('availability_365')

plot_histogram('number_of_reviews')